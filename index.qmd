---
title: "Larger Than Memory Data Workflows with Apache Arrow and Ibis"
execute:
  eval: false
jupyter: python3
---

This the website for the workshop "Larger Than Memory Data Workflows
with Apache Arrow and Ibis" taught at the J on The Beach conference on
May 10th, 2023.


## Abstract

As datasets become larger and more complex, the boundaries between
data engineering and data science are becoming blurred. Data analysis
pipelines with larger-than-memory data are becoming commonplace,
creating a gap that needs to be bridged: between engineering tools
designed to work with very large datasets on the one hand, and data
science tools that provide the analysis capabilities used in data
workflows on the other. One way to build this bridge is with Apache
Arrow, a multi-language toolbox for working with larger-than-memory
tabular data. Arrow is designed to improve performance and efficiency,
and places emphasis on standardization and interoperability among
workflow components, programming languages, and systems. We'll combine
the power of Arrow for compressing data into its most compact form
with Ibis for data analysis in Python. Ibis is a pandas-like, Python
interface that allows you to do robust analytics work on data of any
size. It does this by letting you choose from a range of powerful
database engines that can process your data queries. In this workshop
we'll be working with a very large dataset and drawing insights from
it all from our local machine.

## Introduction 

### Ice-Breaking Activity

Type in the chat:

- what you know about Apache Arrow in one sentence
- what you know about Ibis in one sentence

### What You'll Learn

By the end of this tutorial you will learn: 

- How to access and convert large CSV files into Parquet using Apache Arrow 
- How changing CSV files into parquet compresses them efficiently letting you work with them on your local machine 
- How to analyse larger than memory parquet files using Ibis 
- How to drawing interesting insights from the PUMS census dataset efficiently
- How to switch between and local and remote database using Ibis 

### Demonstration 

Demonstration of what the training is working towards that shows the
power of Arrow to work with large datasets. This should be live coded
but not take more than 5 min.

Specifics: 
- Show graph examples of data compression from turning file from CSV to Parquet Using Arrow.
- Show one example of trying to read the parquet file using pandas (this should break.) Then show and example of reading the parquet file in with Ibis. 

## Overview of the Training

* Importing data in the Apache Arrow ecosystems, what are the
  different file formats, how do they differ, which ones to use?
* Learning how to use Ibis for data wrangling on Parquet files
* Use Ibis with remote data sources: Snowflake (demo to try at home)

## Presentation of the dataset

### What is PUMS?

The Public Use Microdata Sample (PUMS) dataset is a subsample of the American Community Survey (ACS) interviews done on one percent of all US households. It is a detailed collection of information including demographic, housing, and economic characteristics. It is collected by the U.S. Census Bureau as part of the decennial census and is made available to the public for research and analysis. However, due to its large size, analyzing the PUMS dataset is difficult. Today, we will show you how it is possible to wrangle it with just a couple of tools.

Some characteristics of PUMS:

- Public Use: data is anonymized and downloadable 
- Microdata: records of individual people 
- Sample: a representative sample of the population 

![from census.gov](whatispums.png)

The ACS typically produces 1-, and 5-year PUMS files that they make available as SAS and CSV files [here](https://www.census.gov/programs-surveys/acs/microdata/access.2021.html#list-tab-735824205). 


### The dataset

In this tutorial we will be using data from the [PUMS 2016-2020 ACS 5 year dataset](https://www.census.gov/programs-surveys/acs/microdata/access.2020.html#list-tab-735824205). We will be accessing these PUMS files through the File Transfer Protocol (FTP) website. 

The 2020 ACS 5-year PUMS dataset contains a set of two file types:

* housing records 
* person records 

These records represent the fact that PUMS is divided into data about people and data about households. Person data is named `psam_p**.csv`, and household data is named `psam_h**.csv`.

Each file contains over 234 columns, and you can find the column name and the question that was asked to retrieve the information in the [PUMS Data Dictionary](https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMS_Data_Dictionary_2016-2020.pdf). 

For this tutorial we are specifically interested in the following columns from the 2 tables. 

The 2 Tables: 

- **psam_p**.csv (most specialized) (153 MB)**: 
  + HINCP: household income for the past 12 months
  + ST: a list of numbers, corresponding to the various US states (alphabetized)


- **psam_p**.csv (most specialized) (153 MB)**: 
  + WAGP: person income or wages for the past 12 months
  + AGEP: person age 


More dataset info: [https://www.census.gov/programs-surveys/acs/microdata/documentation.2020.html#list-tab-1370939201](https://www.census.gov/programs-surveys/acs/microdata/documentation.2020.html#list-tab-1370939201)

Data source (FTP): [https://www2.census.gov/programs-surveys/acs/data/pums/2020/5-Year/](https://www2.census.gov/programs-surveys/acs/data/pums/2020/5-Year/)

 

In Part 1 of this tutorial we will get the data from the FTP site and and store it on our machine. 


