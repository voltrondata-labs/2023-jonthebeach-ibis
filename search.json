[
  {
    "objectID": "01-part-1.html",
    "href": "01-part-1.html",
    "title": "Part 1 – Dataset",
    "section": "",
    "text": "The Public Use Microdata Sample (PUMS) dataset is a subsample of the American Community Survey (ACS) interviews done on one percent of all US households. It is a detailed collection of information including demographic, housing, and economic characteristics. It is collected by the U.S. Census Bureau as part of the decennial census and is made available to the public for research and analysis. However, due to its large size, analyzing the PUMS dataset is difficult. Today, we will show you how it is possible to wrangle it with just a couple of tools.\nSome characteristics of PUMS:\n\nPublic Use: data is anonymized and downloadable\nMicrodata: records of individual people\nSample: a representative sample of the population\n\n\n\n\nfrom census.gov\n\n\nThe ACS typically produces 1-, and 5-year PUMS files that they make available as SAS and CSV files here.\n\n\n\nIn this tutorial we will be using data from the PUMS 2016-2020 ACS 5 year dataset. We will be accessing these PUMS files through the File Transfer Protocol (FTP) website.\nThe 2020 ACS 5-year PUMS dataset contains a set of two file types:\n\nhousing records\nperson records\n\nThese records represent the fact that PUMS is divided into data about people and data about households. Person data is named psam_p**.csv, and household data is named psam_h**.csv.\nEach file contains over 234 columns, and you can find the column name and the question that was asked to retrieve the information in the PUMS Data Dictionary.\nFor this tutorial we are specifically interested in the following columns from the 2 tables.\nThe 2 Tables:\n\npsam_p**.csv (most specialized) (153 MB):\n\nHINCP: household income for the past 12 months\nST: a list of numbers, corresponding to the various US states (alphabetized)\n\npsam_p**.csv (most specialized) (153 MB):\n\nWAGP: person income or wages for the past 12 months\nAGEP: person age\n\n\nMore dataset info: https://www.census.gov/programs-surveys/acs/microdata/documentation.2020.html#list-tab-1370939201\nData source (FTP): https://www2.census.gov/programs-surveys/acs/data/pums/2020/5-Year/\nIn Part 2 of this tutorial we will get the data from the FTP site and and store it on our machine."
  },
  {
    "objectID": "04-part-4.html",
    "href": "04-part-4.html",
    "title": "Part 4 – Using Ibis With SnowflakeDB",
    "section": "",
    "text": "This the website for the workshop “Larger Than Memory Data Workflows with Apache Arrow and Ibis” taught at the J on The Beach conference on May 10th, 2023."
  },
  {
    "objectID": "04-part-4.html#working-with-remote-data",
    "href": "04-part-4.html#working-with-remote-data",
    "title": "Part 4 – Using Ibis With SnowflakeDB",
    "section": "Working with Remote Data",
    "text": "Working with Remote Data\n\nLearning objectives\n\nConnect to Snowflake database\nShow how code from previous tutorial can be reused with different backend\nMake a table and convert it into a local parquet file.\n\n\n\nIntroduction to Snowflake"
  },
  {
    "objectID": "03-part-3.html",
    "href": "03-part-3.html",
    "title": "Part 3 – Data Wrangling with Ibis",
    "section": "",
    "text": "This the website for the workshop “Larger Than Memory Data Workflows with Apache Arrow and Ibis” taught at the J on The Beach conference on May 10th, 2023.\n\n\n\n\n\n\nLearning objectives\n\n\n\n\n\n\n\nIntroduction to Ibis\nIn this part of the training, we want to do some analytics work to get insights from the data. To do this we will be using Ibis.\nIbis is a Python library that provides a lightweight, universal interface for data wrangling. It helps Python users explore and transform data of any size, stored anywhere.\nTo do this, under the hood Ibis compiles and generates SQL for many backends, including MySQL, PostgresSQL, DuckDB, PySpark, and BigQuery, amongst others. Ibis currently supports 15 backends and counting!\n\n\n\nIbis backends\n\n\nAdditionally, with Ibis, the code used for queries is independent from the backend. If the data are moved to a different system (PostgreSQL, SQLite, Google BigQuery, etc…), the code will not need to be rewritten.\nIbis has excellent read_csv and read_parquet functions that can let you access and analyze data quickly! These methods use DuckDB as their default and will be what we’ll start with today.\nLater you will work with files stored remotely in SnowkflakeDB to see how to switch between backends.\n\n\nAnalysing the Data\nLet’s now use Ibis to wrangle and analyse our data. We’ll start by importing the dependencies we need and reading in the parquet files for the household data.\n\nimport ibis\nfrom ibis import _\nibis.options.interactive = True\nimport matplotlib.pyplot as plt\n\n\nhousehold = ibis.read_parquet(\"partitioned_household/*/*.parquet\",\n                              hive_partitioning=True)\n\nhousehold is now an Ibis table that you can query in a similar way that you would a pandas dataframe. PUMS contains a very long list of columns and the column names are not very intuitive. You can find the full list of columns, they’re names and the data they contain here.\n\n\nOwner and Renter Occupied Housing by Age\nFor this first part of the tutorial we will specifically be using the TEN and HHLDRAGEP columns to create a graph that shows owner and renter occupied housing by age.\nHere are the questions asked to get the data for these columns:\nTEN Character 1\nTenure\nb .N/A (GQ/vacant)\n1 .Owned with mortgage or loan (include home equity loans)\n2 .Owned free and clear\n3 .Rented\n4 .Occupied without payment of rent\nHHLDRAGEP Numeric 2\nAge of the householder\nbb .N/A (GQ/vacant)\n15..99 .15 to 99 years (Top-coded)\nWe’ll start by grabbing the necessary columns, filtering out nulls and relabeling the columns names. Here’s the code to do this:\n\ntenure_type = (\n    household\n    .select('HHLDRAGEP', 'TEN')\n    .filter(_.TEN.notnull())\n    .relabel(dict(\n        HHLDRAGEP=\"age_householder\",\n        TEN=\"tenure_type\",\n    ))\n)\n\n\n\n\n\n\n\nNote\n\n\n\nRun this and print head of this table\n\n\nNext we’ll use mutate to add two new columns tenure_type and age_group to the table. These will be used to group the different types of home ownership and age groups into more clear categories using the group_by function.\n\ntenure_type_age_group = (\n    tenure_type\n    .mutate(\n        tenure_type=ibis.case()\n        .when(tenure_type.tenure_type == 1, \"owned_with_mortgage\")\n        .when(tenure_type.tenure_type == 2, \"owned_free_and_clear\")\n        .when(tenure_type.tenure_type == 3, \"rented\")\n        .when(tenure_type.tenure_type == 4, \"no_rent\")\n        .else_(\"missing\")\n        .end(),\n        age_group=ibis.case()\n        .when(tenure_type.age_householder < 25, \"Under 25 years\")\n        .when((tenure_type.age_householder >= 25) & (tenure_type.age_householder <= 29), \"25 - 29\")\n        .when((tenure_type.age_householder >= 30) & (tenure_type.age_householder <= 34), \"30 - 34\")\n        .when((tenure_type.age_householder >= 35) & (tenure_type.age_householder <= 44), \"35 - 44\")\n        .when((tenure_type.age_householder >= 45) & (tenure_type.age_householder <= 54), \"45 - 54\")\n        .when((tenure_type.age_householder >= 55) & (tenure_type.age_householder <= 64), \"55 - 64\")\n        .when((tenure_type.age_householder >= 65) & (tenure_type.age_householder <= 74), \"65 - 74\")\n        .when(tenure_type.age_householder >= 75, \"75 years and older\")\n        .end()\n    )\n    .group_by([_.age_group, _.tenure_type])\n    .count()\n)\n\nFinally we’ll use plotnine (the Python version of ggplot) to display render the graph\n\npdttag = tenure_type_age_group.to_pyarrow().to_pandas()\npdttag['age_group'] = pd.Categorical(pdttag['age_group'], ordered=True,\n                                     categories=['Under 25 years', '25 - 29', '30 - 34', '35 - 44',\n                                                 '45 - 54', '55 - 64', '65 - 74', '75 years and older'])\n\na = (ggplot(pdttag, aes('age_group', 'count', color = 'tenure_type'))\n + geom_line(aes(group = 'tenure_type')))\na.save('ttt_age_group.pdf', width=29.7, height=21, units='cm')\n\n#Run code to show the graph render\nHopefully, you got a similar graph to the one above. If you’re having any issues let your instructor know!\n\n\nHouseholds with young children and no internet connection at home\nLet’s get more insights from our data! One qustion we can ask is ‘how many households children younger than 15 do not have an internet connection at home in each state?’\nTo answer this question, we’ll start by finding the number of people with children under 15 years old. We’ll also use group_by to get the total number of households per state.\n\nwith_kids = (\n    person\n    .filter(_.AGEP <= 15)\n    .select(_.SERIALNO)\n    .distinct()\n)\n\ntotal_households_by_state = (\n    household\n    .group_by('ST')\n    .count()\n    .relabel({\"count\": \"total_households\"})\n)\n\nNow let’s join the household data with the with_kids table we just created above. We’ll also just return from this table the households without internet connection. These houses are represented by the number 3 in the ACCESSINET column.\n\nno_internet_households_by_state = (\n    household\n    .join(with_kids, with_kids.SERIALNO == _.SERIALNO)\n    .filter((_.ACCESSINET.notnull()) & (_.ACCESSINET == 3))\n    .group_by('ST')\n    .count()\n    .relabel({\"count\": \"household_no_internet\"})\n)\n\nLet’s now return the proportion of the houses in specific states that fall into the data group we’ve just decribed.\n\nres = (\n    total_households_by_state\n    .join(no_internet_households_by_state, no_internet_households_by_state.ST == total_households_by_state.ST)\n    .mutate(\n        prop = (_.household_no_internet/_.total_households) * 100\n    )\n    .join(states, states.pums_code == _.ST)\n)\n\nres.order_by(ibis.desc(_.prop))\n\n\n\n\n\n\n\nNote\n\n\n\nRun this code and show the resulting table.\n\n\nGreat work! For the last part of this tutorial we’ll switch to a remote database and answer a few more questions but using another backend."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Larger Than Memory Data Workflows with Apache Arrow and Ibis",
    "section": "",
    "text": "This the website for the workshop “Larger Than Memory Data Workflows with Apache Arrow and Ibis” taught at the J on The Beach conference on May 10th, 2023."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Larger Than Memory Data Workflows with Apache Arrow and Ibis",
    "section": "Abstract",
    "text": "Abstract\nAs datasets become larger and more complex, the boundaries between data engineering and data science are becoming blurred. Data analysis pipelines with larger-than-memory data are becoming commonplace, creating a gap that needs to be bridged: between engineering tools designed to work with very large datasets on the one hand, and data science tools that provide the analysis capabilities used in data workflows on the other. One way to build this bridge is with Apache Arrow, a multi-language toolbox for working with larger-than-memory tabular data. Arrow is designed to improve performance and efficiency, and places emphasis on standardization and interoperability among workflow components, programming languages, and systems. We’ll combine the power of Arrow for compressing data into its most compact form with Ibis for data analysis in Python. Ibis is a pandas-like, Python interface that allows you to do robust analytics work on data of any size. It does this by letting you choose from a range of powerful database engines that can process your data queries. In this workshop we’ll be working with a very large dataset and drawing insights from it all from our local machine."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Larger Than Memory Data Workflows with Apache Arrow and Ibis",
    "section": "Introduction",
    "text": "Introduction\n\nIce-Breaking Activity\nType in the chat:\n\nwhat you know about Apache Arrow in one sentence\nwhat you know about Ibis in one sentence\n\n\n\nWhat You’ll Learn\nBy the end of this tutorial you will learn:\n\nHow to access and convert large CSV files into Parquet using Apache Arrow\nHow changing CSV files into parquet compresses them efficiently letting you work with them on your local machine\nHow to analyse larger than memory parquet files using Ibis\nHow to drawing interesting insights from the PUMS census dataset efficiently\nHow to switch between and local and remote database using Ibis\n\n\n\nDemonstration\nDemonstration of what the training is working towards that shows the power of Arrow to work with large datasets. This should be live coded but not take more than 5 min.\nSpecifics: - Show graph examples of data compression from turning file from CSV to Parquet Using Arrow. - Show one example of trying to read the parquet file using pandas (this should break.) Then show and example of reading the parquet file in with Ibis."
  },
  {
    "objectID": "index.html#overview-of-the-training",
    "href": "index.html#overview-of-the-training",
    "title": "Larger Than Memory Data Workflows with Apache Arrow and Ibis",
    "section": "Overview of the Training",
    "text": "Overview of the Training\n\nImporting data in the Apache Arrow ecosystems, what are the different file formats, how do they differ, which ones to use?\nLearning how to use Ibis for data wrangling on Parquet files\nUse Ibis with remote data sources: Snowflake (demo to try at home)"
  },
  {
    "objectID": "02-part-2.html",
    "href": "02-part-2.html",
    "title": "Part 2 – Data Import And CSV To Parquet Conversion",
    "section": "",
    "text": "This the website for the workshop “Larger Than Memory Data Workflows with Apache Arrow and Ibis” taught at the J on The Beach conference on May 10th, 2023."
  },
  {
    "objectID": "02-part-2.html#accessing-the-data",
    "href": "02-part-2.html#accessing-the-data",
    "title": "Part 2 – Data Import And CSV To Parquet Conversion",
    "section": "Accessing the Data",
    "text": "Accessing the Data\n\nThe PUMS Dataset\nThe U.S. Census provides the PUMS data in several ways. You can browse the data right on their website. But after selecting 10 variables (out of 522) we are met with a throttle limit on how much data we can select at a time.\n\n\n\nhttps://www.census.gov/programs-surveys/acs/microdata.html\n\n\nLuckily for us, census.gov also provides data via their FTP server. We’ll be using the five-year data from 2020, found here.\nWe may have already downloaded the data for you, check with your instructor! If this is the case skip to this section.\n\n\nDownloading the Data\nYou can download the data in CSV format using this FTP URL: ftp://ftp.census.gov/programs-surveys/acs/data/pums/2020/5-Year/. To do this use your preferred tool, such as wget or aria2.\nAs an example, here’s how to download the data using wget on a Mac. Run the following lines in your terminal, to retrieve and then unzip the data:\n# use wget or your ftp retrieval method of choice \nwget --recursive -w 2 -nc --level=1 --no-parent --no-directories \\\n     --accept 'csv*' --reject '*pus*' --reject '*hus*' \\\n     --directory-prefix=. \\\n     ftp://ftp.census.gov/programs-surveys/acs/data/pums/2020/5-Year/\n\n# unizp the files, feel free to use your method of choice \nunzip *.zip\nThe PUMS dataset is divided into data about people and data about households. After unzipping the files, we’ll divide the five-year set into person-focused and housing-focused data.\nPerson data can be identified by the naming pattern psam_p*.csv, while housing data has the pattern psam_h*.csv. From there, you can just use globbing to move them where you want them — the example below will work on Mac and Linux.\nmkdir housing\nmkdir people\nmv psam_h*.csv ./household/\nmv psam_p*.csv ./people/\n\n\nExamining the Data\nOnce this is done, you should now have two directories full of CSV data that make up approximately 14.13 GB.\n\n\n\n\n\n\nActivity\n\n\n\n\nCheck the size of the two directories on your machine, are they also 14GB?\n\n\n\n14 GB is far too big for a desktop environment, you will run out of RAM working with this data. Additionally, there are too many files in each directory to wrangle efficiently.\nIn our workshop today we’ll convert the CSV files into two Parquet files: one for the household data, and one for person data. This will compress the data down to 4 GB! The Python Library PyArrow is well suited for this task.\n\n\nIntroduction to Apache Arrow\n[In this section we need more info about Arrow and Parquet]\nBefore we move further here are a few things to know about Arrow:\n\nIt is a columnar data-format and a standard.\nIt can be used to convert one format to another\nMany programming languages have their own library to work with Arrow. We are using PyArrow today, but others exist for C++, Go, R, Rust, etc.\n\nLet’s import PyArrow and the specific methods we need:\n\nimport pyarrow as pa\nfrom pyarrow import parquet\nimport pyarrow.dataset as ds\n\nNow, we need a view of the data — we’ll use PyArrow’s dataset functionality to do this.\nA view does not read any data into memory.\n\n# This line takes all the csvs files in the folder and adds them to the\n# same dataset.\n\npeople_dataset = ds.dataset(\"./people\", format=\"csv\")\nhouse_dataset = ds.dataset(\"./household\", format=\"csv\")\n\nContrary to CSV files, arrow datasets are typed. When using the default options, the data types are inferred by reading a subset of the data to guess the types. However, with large datasets, it can happen that a variable is detected as being numeric when it contains strings outside the range used to infer the data type.\nTo handle this, we will redefine the schema and make a new view. This is a cheap operation because we haven’t read any data yet, only set up a view that’s aware of the files.\n\n# Adjust the person data schema   \npeople_schema = people_dataset.schema.set(1, pa.field(\"SERIALNO\", pa.string()))\npeople_schema = people_schema.set(75, pa.field(\"WKWN\", pa.string()))\n\n# Adjust the household data schema \nhouse_schema = house_dataset.schema.set(1, pa.field(\"SERIALNO\", pa.string()))\n\n#Update views with new schemas \npeople_dataset = ds.dataset('./people', format=\"csv\", schema=people_schema)\n\nhouse_dataset = ds.dataset('./household', format=\"csv\", schema=house_schema)\n\nNow, we write to Parquet. This can take a while, as it’s only now that PyArrow fully reads the data.\n\nds.write_dataset(house_dataset, \"pums_household\", format=\"parquet\")\nds.write_dataset(people_dataset, \"pums_people\", format=\"parquet\")\n\n\n\nThe Results\nWhat gains do we get from this?\nCSV to Parquet file size changes:\n\nPerson Data: 10 GB –> 3.3GB\nHousehold Data: 4 GB –> 1.3 GB\n\nThat’s a 3X reduction for both of these in a few lines of code!\nParquet compresses data, but unlike zip files, it’s format allows data to be read directly in that compressed form.\nIn the next part of the tutorial we’ll be using Ibis to read in our newly made parquet files and analyze the data to draw insights from it!"
  }
]