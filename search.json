[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Larger Than Memory Data Workflows with Apache Arrow and Ibis",
    "section": "",
    "text": "This the website for the workshop “Larger Than Memory Data Workflows with Apache Arrow and Ibis” taught at the J on The Beach conference on May 10th, 2023."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Larger Than Memory Data Workflows with Apache Arrow and Ibis",
    "section": "Abstract",
    "text": "Abstract\nAs datasets become larger and more complex, the boundaries between data engineering and data science are becoming blurred. Data analysis pipelines with larger-than-memory data are becoming commonplace, creating a gap that needs to be bridged: between engineering tools designed to work with very large datasets on the one hand, and data science tools that provide the analysis capabilities used in data workflows on the other. One way to build this bridge is with Apache Arrow, a multi-language toolbox for working with larger-than-memory tabular data. Arrow is designed to improve performance and efficiency, and places emphasis on standardization and interoperability among workflow components, programming languages, and systems. We’ll combine the power of Arrow for compressing data into its most compact form with Ibis for data analysis in Python. Ibis is a pandas-like, Python interface that allows you to do robust analytics work on data of any size. It does this by letting you choose from a range of powerful database engines that can process your data queries. In this workshop we’ll be working with a very large dataset and drawing insights from it all from our local machine."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Larger Than Memory Data Workflows with Apache Arrow and Ibis",
    "section": "Introduction",
    "text": "Introduction\n\nIce-Breaking Activity\nType in the chat:\n\nwhat you know about Apache Arrow in one sentence\nwhat you know about Ibis in one sentence\n\n\n\nWhat You’ll Learn\nBy the end of this tutorial you will learn:\n\nHow to access and convert large CSV files into Parquet using Apache Arrow\nHow changing CSV files into parquet compresses them efficiently letting you work with them on your local machine\nHow to analyse larger than memory parquet files using Ibis\nHow to drawing interesting insights from the PUMS census dataset efficiently\nHow to switch between and local and remote database using Ibis\n\n\n\nDemonstration\nDemonstration of what the training is working towards that shows the power of Arrow to work with large datasets. This should be live coded but not take more than 5 min.\nSpecifics: - Show graph examples of data compression from turning file from CSV to Parquet Using Arrow. - Show one example of trying to read the parquet file using pandas (this should break.) Then show and example of reading the parquet file in with Ibis."
  },
  {
    "objectID": "index.html#overview-of-the-training",
    "href": "index.html#overview-of-the-training",
    "title": "Larger Than Memory Data Workflows with Apache Arrow and Ibis",
    "section": "Overview of the Training",
    "text": "Overview of the Training\n\nImporting data in the Apache Arrow ecosystems, what are the different file formats, how do they differ, which ones to use?\nLearning how to use Ibis for data wrangling on Parquet files\nUse Ibis with remote data sources: Snowflake (demo to try at home)\n\n\n\n\nDiagram showing the components that will be covered during the workshop (black arrows). Ibis uses different engines to interact with the data (in orange). Dashed, gray arrows show things that are possible but not covered in the workshop."
  },
  {
    "objectID": "01-part-1.html",
    "href": "01-part-1.html",
    "title": "Part 1 – Introduction to the Dataset",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nGetting familiar with what the ACS PUMS data is and how it is structured\nUnderstanding the questions we are going to explore in this tutorial"
  },
  {
    "objectID": "01-part-1.html#presentation-of-the-dataset",
    "href": "01-part-1.html#presentation-of-the-dataset",
    "title": "Part 1 – Introduction to the Dataset",
    "section": "Presentation of the dataset",
    "text": "Presentation of the dataset\n\nWhat is PUMS?\nThe Public Use Microdata Sample (PUMS) dataset is a subsample of the American Community Survey (ACS) interviews done on one percent of all US households. It is a detailed collection of information including demographic, housing, and economic characteristics. It is collected by the U.S. Census Bureau as part of the decennial census and is made available to the public for research and analysis. However, due to its large size, analyzing the PUMS dataset is difficult. Today, we will show you how it is possible to wrangle it with just a couple of tools.\nSome characteristics of PUMS:\n\nPublic Use: data is anonymized, in the public domain, and downloadable\nMicrodata: records of individual people\nSample: a representative sample of the population\n\n\n\n\nfrom census.gov\n\n\nThe ACS typically produces 1-, and 5-year PUMS files that they make available as SAS and CSV files.\n\n\nThe dataset\nIn this tutorial we will be using data from the PUMS 2016-2020 ACS 5 year dataset. We will be accessing these PUMS files through the File Transfer Protocol (FTP).\nThe 2020 ACS 5-year PUMS dataset contains a set of two file types:\n\nhousing records\nperson records\n\nThese records represent the fact that PUMS is divided into data about people and data about households. Person data is named psam_p**.csv, and household data is named psam_h**.csv. The household and person data can be joined using the SERIALNO field. Each household is identified with a unique SERIALNO, and each person is identified by the combination of the SERIALNO and SPORDER values.\nBecause the PUMS data is a sample of 1% of the population, the data includes weights that can be used to estimate the number of people the observation represents. For this tutorial, we do not take these weights into account as we are focusing on how to work with data rather than exact demographic estimates. If you would like to learn more about using the weights included in the ACS PUMS data to calculate more accurate estimates, you can refer to this document from the US Census Bureau.\nEach file contains over 234 columns, and you can find the column name and the question that was asked to retrieve the information in the PUMS Data Dictionary.\nIn this tutorial, we are going to explore how the proportion of people owning and renting their house changes with age. We will also calculate the proportion of households with children under the age of 15 that do not have internet access.\nFor this, we will use the following columns from the household and person datasets:\n\npsam_h**.csv (household data) (4 GB):\n\nTEN: The type of tenure: whether the people living in the house are renting, owning, or occupying without payment of rent.\nHHLDRAGEP: The age of the householder.\nST: a list of numbers, corresponding to the various US states and Puerto Rico (alphabetized).\nACCESSINET: whether the household has internet access.\n\npsam_p**.csv (person data, most specialized) (10.1 GB):\n\nAGEP: The person age.\n\n\nWe are also providing a table that we can use to convert the numerically-encoded US states into their 2-letter abbreviations.\nMore dataset info: https://www.census.gov/programs-surveys/acs/microdata/documentation.2020.html#list-tab-1370939201\nData source (FTP): https://www2.census.gov/programs-surveys/acs/data/pums/2020/5-Year/\nIn Part 2 of this tutorial we will get the data from the FTP site and and store it on our machine."
  },
  {
    "objectID": "02-part-2.html",
    "href": "02-part-2.html",
    "title": "Part 2 – Data Import And CSV To Parquet Conversion",
    "section": "",
    "text": "This the website for the workshop “Larger Than Memory Data Workflows with Apache Arrow and Ibis” taught at the J on The Beach conference on May 10th, 2023."
  },
  {
    "objectID": "02-part-2.html#accessing-the-data",
    "href": "02-part-2.html#accessing-the-data",
    "title": "Part 2 – Data Import And CSV To Parquet Conversion",
    "section": "Accessing the Data",
    "text": "Accessing the Data\n\nThe PUMS Dataset\nThe U.S. Census provides the PUMS data in several ways. You can browse the data right on their website. But after selecting 10 variables (out of 522) we are met with a throttle limit on how much data we can select at a time.\n\n\n\nhttps://www.census.gov/programs-surveys/acs/microdata.html\n\n\nLuckily for us, census.gov also provides data via their FTP server. We’ll be using the five-year data from 2020, found here.\nWe may have already downloaded the data for you, check with your instructor! If this is the case skip to this section.\n\n\nDownloading the Data\nYou can download the data in CSV format using this FTP URL: ftp://ftp.census.gov/programs-surveys/acs/data/pums/2020/5-Year/. To do this use your preferred tool, such as wget or aria2.\nAs an example, here’s how to download the data using wget on a Mac. Run the following lines in your terminal, to retrieve and then unzip the data:\n# use wget or your ftp retrieval method of choice \nwget --recursive -w 2 -nc --level=1 --no-parent --no-directories \\\n     --accept 'csv*' --reject '*pus*' --reject '*hus*' \\\n     --directory-prefix=. \\\n     ftp://ftp.census.gov/programs-surveys/acs/data/pums/2020/5-Year/\n\n# unizp the files, feel free to use your method of choice \nunzip *.zip\nThe PUMS dataset is divided into data about people and data about households. After unzipping the files, we’ll divide the five-year set into person-focused and housing-focused data.\nPerson data can be identified by the naming pattern psam_p*.csv, while housing data has the pattern psam_h*.csv. From there, you can just use globbing to move them where you want them — the example below will work on Mac and Linux.\nmkdir housing\nmkdir people\nmv psam_h*.csv ./household/\nmv psam_p*.csv ./people/\n\n\nExamining the Data\nOnce this is done, you should now have two directories full of CSV data that make up approximately 14.13 GB.\n\n\n\n\n\n\nActivity\n\n\n\n\nCheck the size of the two directories on your machine, are they also 14GB?\n\n\n\n14 GB is far too big for a desktop environment, you will run out of RAM working with this data. Additionally, there are too many files in each directory to wrangle efficiently.\nIn our workshop today we’ll convert the CSV files into two Parquet files: one for the household data, and one for person data. This will compress the data down to 4 GB! The Python Library PyArrow is well suited for this task.\n\n\nIntroduction to Apache Arrow\n[In this section we need more info about Arrow and Parquet]\nBefore we move further here are a few things to know about Arrow:\n\nIt is a columnar data-format and a standard.\nIt can be used to convert one format to another\nMany programming languages have their own library to work with Arrow. We are using PyArrow today, but others exist for C++, Go, R, Rust, etc.\n\nLet’s import PyArrow and the specific methods we need:\n\nimport pyarrow as pa\nfrom pyarrow import parquet\nimport pyarrow.dataset as ds\n\nNow, we need a view of the data — we’ll use PyArrow’s dataset functionality to do this.\nA view does not read any data into memory.\n\n# This line takes all the csvs files in the folder and adds them to the\n# same dataset.\n\npeople_dataset = ds.dataset(\"./people\", format=\"csv\")\nhouse_dataset = ds.dataset(\"./household\", format=\"csv\")\n\nContrary to CSV files, arrow datasets are typed. When using the default options, the data types are inferred by reading a subset of the data to guess the types. However, with large datasets, it can happen that a variable is detected as being numeric when it contains strings outside the range used to infer the data type.\nTo handle this, we will redefine the schema and make a new view. This is a cheap operation because we haven’t read any data yet, only set up a view that’s aware of the files.\n\n# Adjust the person data schema   \npeople_schema = people_dataset.schema.set(1, pa.field(\"SERIALNO\", pa.string()))\npeople_schema = people_schema.set(75, pa.field(\"WKWN\", pa.string()))\n\n# Adjust the household data schema \nhouse_schema = house_dataset.schema.set(1, pa.field(\"SERIALNO\", pa.string()))\n\n#Update views with new schemas \npeople_dataset = ds.dataset('./people', format=\"csv\", schema=people_schema)\n\nhouse_dataset = ds.dataset('./household', format=\"csv\", schema=house_schema)\n\nNow, we write to Parquet. This can take a while, as it’s only now that PyArrow fully reads the data.\n\nds.write_dataset(house_dataset, \"pums_household\", format=\"parquet\")\nds.write_dataset(people_dataset, \"pums_people\", format=\"parquet\")\n\n\n\nThe Results\nWhat gains do we get from this?\nCSV to Parquet file size changes:\n\nPerson Data: 10 GB –> 3.3GB\nHousehold Data: 4 GB –> 1.3 GB\n\nThat’s a 3X reduction for both of these in a few lines of code!\nParquet compresses data, but unlike zip files, it’s format allows data to be read directly in that compressed form.\nIn the next part of the tutorial we’ll be using Ibis to read in our newly made parquet files and analyze the data to draw insights from it!"
  },
  {
    "objectID": "04-part-4.html",
    "href": "04-part-4.html",
    "title": "Part 4 – Using Ibis With SnowflakeDB",
    "section": "",
    "text": "This the website for the workshop “Larger Than Memory Data Workflows with Apache Arrow and Ibis” taught at the J on The Beach conference on May 10th, 2023."
  },
  {
    "objectID": "04-part-4.html#working-with-remote-data",
    "href": "04-part-4.html#working-with-remote-data",
    "title": "Part 4 – Using Ibis With SnowflakeDB",
    "section": "Working with Remote Data",
    "text": "Working with Remote Data\n\nLearning objectives\n\nConnect to Snowflake database\nShow how code from previous tutorial can be reused with different backend\nMake a table and convert it into a local parquet file.\n\n\n\nIntroduction to Snowflake"
  },
  {
    "objectID": "03-part-3.html",
    "href": "03-part-3.html",
    "title": "Part 3 – Data Wrangling with Ibis",
    "section": "",
    "text": "Learning objectives\n\n\n\n\nExplain the value of Ibis for data analysis\nWrite data queries using Ibis to get insight from data\nApply chaining to combine operations\n\n\n\n\nIntroduction to Ibis\nIn this part of the training, we want to do some analytics work to get insights from the data. To do this we will be using Ibis.\nIbis is a Python library that provides a lightweight, universal interface for data wrangling. It helps Python users explore and transform data of any size, stored anywhere.\nTo do this, under the hood Ibis compiles and generates SQL for many backends, including MySQL, PostgresSQL, DuckDB, PySpark, and BigQuery, amongst others. Ibis currently supports 15 backends and counting!\n\n\n\nIbis backends\n\n\nAdditionally, with Ibis, the code used for queries is independent from the backend. If the data are moved to a different system (PostgreSQL, SQLite, Google BigQuery, etc…), the code will not need to be rewritten.\nIbis has excellent read_csv and read_parquet functions that can let you access and analyze data quickly! These methods use the DuckDB engine as their default and will be what we’ll start with today.\nLater you will work with data stored remotely in Snowkflake to see how to switch between backends.\n\n\nAnalysing the Data\nLet’s now use Ibis to wrangle and analyse our data. We’ll start by importing the dependencies we need and reading in the parquet files for the household data.\n\nimport ibis\nfrom ibis import _\nibis.options.interactive = True\nimport pandas as pd\nfrom plotnine import ggplot, geom_line, aes\n\n\nhousehold = ibis.read_parquet(\"pums_household/*/*.parquet\",\n                              hive_partitioning=True)\n\nhousehold is now an Ibis table that you can query in a similar way that you would a pandas dataframe. PUMS contains a very long list of columns and the column names are not very intuitive. You can find the full list of columns, they’re names and the data they contain here.\n\n\nOwner and Renter Occupied Housing by Age\nFor this first part of the tutorial we will specifically be using the TEN and HHLDRAGEP columns to create a graph that shows owner and renter occupied housing by age.\nHere are the questions asked to get the data for these columns:\nTEN Character 1\nTenure\nb. N/A (GQ/vacant)\n1. Owned with mortgage or loan (include home equity loans)\n2. Owned free and clear\n3. Rented\n4. Occupied without payment of rent\nHHLDRAGEP Numeric 2\nAge of the householder\nbb. N/A (GQ/vacant)\n15..99. 15 to 99 years (Top-coded)\nWe’ll start by grabbing the necessary columns, filtering out nulls and relabeling the columns names. Here’s the code to do this:\n\ntenure_type = (\n    household\n    .select('HHLDRAGEP', 'TEN')\n    .filter(_.TEN.notnull())\n    .relabel(dict(\n        HHLDRAGEP=\"age_householder\",\n        TEN=\"tenure_type\",\n    ))\n)\n\n\n\n\n\n\n\nNote\n\n\n\nRun this and print head of this table\n\n\nNext we’ll use mutate to add two new columns tenure_type and age_group to the table. These will be used to:\n\nrecode the different types of home ownership, and;\nbin the householder ages into groups.\n\nThis operation will allow us to have clearer categories when using the group_by function to calculate the number of people in each group.\n\ntenure_type_age_group = (\n    tenure_type\n    .mutate(\n        tenure_type=ibis.case()\n        .when(tenure_type.tenure_type == 1, \"owned_with_mortgage\")\n        .when(tenure_type.tenure_type == 2, \"owned_free_and_clear\")\n        .when(tenure_type.tenure_type == 3, \"rented\")\n        .when(tenure_type.tenure_type == 4, \"no_rent\")\n        .else_(\"missing\")\n        .end(),\n        age_group=ibis.case()\n        .when(tenure_type.age_householder < 25, \"Under 25 years\")\n        .when((tenure_type.age_householder >= 25) & (tenure_type.age_householder <= 29), \"25 - 29\")\n        .when((tenure_type.age_householder >= 30) & (tenure_type.age_householder <= 34), \"30 - 34\")\n        .when((tenure_type.age_householder >= 35) & (tenure_type.age_householder <= 44), \"35 - 44\")\n        .when((tenure_type.age_householder >= 45) & (tenure_type.age_householder <= 54), \"45 - 54\")\n        .when((tenure_type.age_householder >= 55) & (tenure_type.age_householder <= 64), \"55 - 64\")\n        .when((tenure_type.age_householder >= 65) & (tenure_type.age_householder <= 74), \"65 - 74\")\n        .when(tenure_type.age_householder >= 75, \"75 years and older\")\n        .end()\n    )\n    .group_by([_.age_group, _.tenure_type])\n    .count()\n)\n\nFinally we’ll use plotnine (the Python version of ggplot) to render the graph:\n\npdttag = tenure_type_age_group.to_pyarrow().to_pandas()\npdttag['age_group'] = pd.Categorical(pdttag['age_group'], ordered=True,\n                                     categories=['Under 25 years', '25 - 29', '30 - 34', '35 - 44',\n                                                 '45 - 54', '55 - 64', '65 - 74', '75 years and older'])\n\na = (ggplot(pdttag, aes('age_group', 'count', color = 'tenure_type'))\n + geom_line(aes(group = 'tenure_type')))\na.save('tenure_by_age_group.pdf', width=29.7, height=21, units='cm')\n\n\n\n\n\n\n\n\nActivity\n\n\n\nRun code to show the graph render\n\n\nHopefully, you got a similar graph to the one above. If you’re having any issues let your instructor know!\n\n\nHouseholds with young children and no internet connection at home\nLet’s get more insights from our data! One question we can ask is `What is the proportion of households with children younger than 15 do not have an internet connection at home in each state?’\nTo answer this question, we will need to use the data both from the person and the housing datasets. The person dataset has the age of all the respondants to the survey, while the housing dataset has the information about whether the house has an internet connection. The two datasets can be joined on the SERIALNO column.\nFirst, we’ll start by finding the households that have children 15 years old or younger (with_kids).\n\nperson = ibis.read_parquet(\"pums_people/*/*.parquet\",\n                           hive_partitioning=True)\n\nwith_kids = (\n    person\n    .filter(_.AGEP <= 15)\n    .select(\"SERIALNO\", \"ST\")\n    .distinct()\n)\n\nNow that we have identified the households with children who are less than 15 years old, let’s estimate how many of them do not have an internet connection. To this end, we are going to join the with_kids table we just created with the household table using the SERIALNO field. We can then filter the dataset to only keep the households with no internet access represented with “3” in the ACCESSINET column.\n\nno_internet_households_by_state = (\n    household\n    .join(with_kids, with_kids.SERIALNO == _.SERIALNO)\n    .filter((_.ACCESSINET.notnull()) & (_.ACCESSINET == 3))\n    .drop('ST_y')\n    .relabel({\"ST_x\": \"ST\"})\n    .group_by('ST')\n    .count()\n    .relabel({\"count\": \"household_no_internet\"})\n)\n\nThen, we will tally up the total number of households with children under 15 for each state:\n\ntotal_households_by_state = (\n    with_kids\n    .group_by('ST')\n    .count()\n    .relabel({\"count\": \"total_households\"})\n)\n\nWe can now return the proportion of the houses with children that do not have internet access. We will also join the pums_state table which contains the names of each state so we can more easily interpret the results.\n\nstates = ibis.read_csv(\"pums_states.csv\").mutate(pums_code = _.pums_code.cast('string'))\nres = (\n    total_households_by_state\n    .join(no_internet_households_by_state, no_internet_households_by_state.ST == total_households_by_state.ST)\n    .mutate(\n        prop = (_.household_no_internet/_.total_households) * 100\n    )\n    .join(states, states.pums_code == _.ST)\n)\n\nres.order_by(ibis.desc(_.prop))\n\n\n\n\n\n\n\nActivity\n\n\n\nRun this code and show the resulting table.\n\n\nGreat work! For the last part of this tutorial we’ll switch to a remote database and answer a few more questions but using another backend."
  }
]